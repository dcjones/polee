
% TODO:
% * Actually describe the transform
% * Gradients for  log |J|
% * Fix some numbering

\documentclass{article}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}
\newcommand{\defeq}{\vcentcolon=}

\newtheorem{thm}{Theorem}
%\newtheorem{thm}{Definition}

\begin{document}


\section{Simplex transform $S$}

%% TODO: actually describe this

For $i < n$,
$$z_i = \text{logit}^{-1}\left(y_i + \frac{1}{n - i}\right)$$

For $i < n$,
$$x_i = z_i \left(1 - \sum_{k=1}^{i-1} x_k \right)$$

And for $x_n$,
$$x_n = \left(1 - \sum_{k=1}^{i-1} x_k \right)$$


\subsection{Gradients of $S$}


There are four cases to consider when deriving $\frac{x_j}{y_i}$: $j < i$, $j =
i$, $i < j < n$, and  $j = n$.


For each we'll need the derivative of the intermediate values $z$.
\begin{equation}
\frac{\partial z_i}{\partial y_i} = z_i (1 - z_i)
\end{equation}
For any $i \neq j$ $\partial z_j / \partial y_i = 0$.


\subsubsection{$j < i$}
This case is simply 0. Intuitively, in a stick breaking process, a break cannot
depend on another break that occurs after it. More formally, this can be shown
by induction.

First, note that for any $i > 1$,
$$\frac{\partial x_1}{\partial y_i} = \frac{\partial}{\partial
y_i} z_1 \left(1 - \sum_{k=1}^0 x_k \right) = \frac{\partial}{\partial y_i} z_1 = 0$$

Now, for any $1 < k < j < i < n$, assume $\frac{\partial x_k}{\partial y_i} = 0$.

$$\frac{\partial x_j}{\partial y_i} = \frac{\partial}{\partial y_i} z_j \left(1 -
\sum_{k=1}^{j-1} x_k \right) = z_j \sum_{k=1}^{j-1} \frac{\partial x_k}{\partial y_i} = 0
$$

By induction $\frac{\partial x_j}{\partial y_i} = 0$ holds for all $j < i$.


\subsubsection{$j = i$}

$$\frac{\partial x_i}{\partial y_i} = z_i \left( 1 - \sum_{k=1}^{i-1} x_k \right) =
\frac{\partial z_i}{\partial y_i} \left( 1 - \sum_{k=1}^{i-1} x_k \right) =
z_i (1 - z_i) \left( 1 - \sum_{k=1}^{i-1} x_k \right)$$

\subsubsection{$i < j < n$}

$$\frac{\partial x_j}{\partial y_i} =  - z_j \sum_{k=1}^{j-1}
\frac{\partial x_k}{\partial y_i} =
-z_j \sum_{k=i}^{j-1}
\frac{\partial x_k}{\partial y_i}
$$

Defined as a recurrence relation, it's not obvious that this can be efficiently
computed. However, we can use the following fact to derive a more convenient
form:

\begin{equation}
\frac{\partial x_j}{\partial x_i} = -z_j \prod_{k=i+1}^{j-1} (1 - z_k)
\end{equation}

\begin{proof}
We proceed by induction. First, note that $$
\frac{\partial x_{i+1}}{\partial x_i}
= -z_{i+1} \sum_{k=i}^{i}\frac{\partial x_k}{\partial x_i}
= -z_{i+1} \frac{\partial{x_i}}{\partial x_i}
= -z_{i+1}
= -z_{i+1} \prod_{k=i+1}^{i} (1 - z_k)
$$

Now for any $i, j$ where $i + 1 < j < n$, assume that $\frac{\partial
x_{j-1}}{\partial x_i} =
-z_{j-1} \prod_{i+1}^{j-2} (1 - z_k)$.

\begin{align*}
\frac{\partial x_j}{\partial x_i} &= -z_j \sum_{k=i}^{j-1} \frac{\partial
x_k}{\partial x_i} \\
&= -z_j \left( \frac{\partial x_{j-1}}{\partial x_i} + \sum_{k=i}^{j-2} \frac{\partial
x_k}{\partial x_i} \right) \\
&= -z_j \left( \frac{\partial x_{j-1}}{\partial x_i} - \frac{1}{z_{j-1}} \frac{\partial
x_{j-1}}{\partial x_i} \right) \\
&= -z_j \left( 1 - \frac{1}{z_{j-1}} \right) \frac{\partial x_{j-1}}{\partial
x_i} \\
&= -z_j ( 1 - z_{j-1} ) \prod_{k=i+1}^{j-2} (1-x_k) \\
&= -z_j \prod_{k=i+1}^{j-1} (1-x_k) \\
\end{align*}

Therefore, by induction, the theorem is true for all $i < j < n$.

\end{proof}

From here, we can get the desired gradient in form that's clearly computable in
$O(n)$.
\begin{equation} \label{eq:1}
\frac{\partial x_j}{\partial y_i}
= \frac{\partial x_i}{\partial y_i} \frac{\partial x_j}{\partial x_i}
= - z_i z_j (1 - z_i) \left(1 - \sum_{k=1}^{i-1} x_k \right) \prod_{k=i+1}^{j-1} (1 - z_k)
\end{equation}





\subsubsection{$i = n$}

Since $x_n$ is a special case, consisting of whatever is leftover of the stick
after $n-1$ breaks, it has its own gradient.

$$\frac{\partial x_n}{\partial y_i}
= \frac{\partial}{\partial y_i} \left( 1 - \sum_{k=1}^{n-1} x_k \right)
= - \sum_{k=i}^{n-1} \frac{\partial x_k}{\partial y_i}
$$

Dividing both sides of Equation \ref{eq:1} by $-z_j$ we see that
$$\sum_{k=i}^{j-1} \frac{\partial x_k}{\partial x_i} = \prod_{k=i+1}^{j-1} (1 - z_k)$$

Using this fact, we can simplify our gradient,
\begin{align}
\frac{\partial x_n}{\partial y_i}
&= - \frac{\partial x_i}{\partial y_i} \sum_{k=i}^{n-1} \frac{\partial
x_k}{\partial x_i} \\
&= - \frac{\partial x_i}{\partial y_i} \prod_{k=i+1}^{n-1} (1 - z_k) \\
&= - z_i (1 - z_i) \left( 1 - \sum_{k=1}^{i-1} x_k \right) \prod_{k=i+1}^{n-1} (1 - z_k)
\end{align}


\subsection{Computing Efficiently}

At this point we've seen that $\frac{\partial x_j}{\partial y_i}$ can be
computed in worst case $O(n)$. However, to compute the gradients of an objective
function $f(x)$ (e.g. likelihood).

Fortunately by pre-computing some vectors, $\nabla_y f(x) = \left( \sum_{j=1}^{n-1}
\frac{\partial x_j}{\partial y_i} \frac{\partial f(x)}{\partial x_j} \right)_i$
itself is computable in $O(n)$, though a naive implementation would be $O(n^3)$
even after transforming the recurrence relations to products.

Put another way, the goal should be to compute the following in constant time,
given some vectors that can be pre-computed in $O(n)$, and assuming we are given
$\nabla_x f(x)$ (that is, that the untransformed gradients are already
computed).

$$\left( \nabla_y f(x) \right)_i = \sum_{j=1}^{n} \frac{\partial x_j}{\partial
y_i} \frac{\partial f(x)}{\partial x_j}$$

Since $\frac{\partial x_j}{\partial x_i} = 0$ for $j < i$.

$$\left( \nabla_y f(x) \right)_i = \sum_{j=i}^{n} \frac{\partial x_j}{\partial
y_i} \frac{\partial f(x)}{\partial x_j}$$

Now, pulling out the $j=i$ and $j=n$ cases gives us:
\begin{align*}
\left( \nabla_y f(x) \right)_i &=
\frac{\partial x_i}{\partial y_i} \frac{\partial f(x)}{\partial x_i} +
\frac{\partial x_n}{\partial y_i} \frac{\partial f(x)}{\partial x_n} +
\sum_{j=i+1}^{n-1} \frac{\partial x_j}{\partial
y_i} \frac{\partial f(x)}{\partial x_j} \\
&=
\frac{\partial x_i}{\partial y_i} \left(
\frac{\partial f(x)}{\partial x_i} +
\frac{\partial x_n}{\partial x_i} \frac{\partial f(x)}{\partial x_n} +
\sum_{j=i+1}^{n-1} \frac{\partial x_j}{\partial
x_i} \frac{\partial f(x)}{\partial x_j} \right) \\
&=
\frac{\partial x_i}{\partial y_i} \left(
\frac{\partial f(x)}{\partial x_i} -
\frac{\partial f(x)}{\partial x_n} \prod_{k=i+1}^{n-1} (1 - z_k)
- \sum_{j=i+1}^{n-1} \frac{\partial f(x)}{\partial x_j}
z_j \prod_{k=i+1}^{j-1} (1 - z_k) \right) \\
\end{align*}

To simplify this further we define two vectors of length $n$, $u$ and $v$.
$$u_i \defeq \prod_{k=1}^{i} (1 - z_k)$$
$$v_i \defeq
\sum_{j=1}^{i} \frac{\partial f(x)}{\partial x_j} z_j \prod_{k=1}^{j-1} (1 - z_k)
= \sum_{j=1}^{i} \frac{\partial f(x)}{\partial x_j} z_j u_{j-1}
$$

Note that both of these can be computed in $O(n)$. This lets us reduce the
gradient further:
\begin{align*}
\left( \nabla_y f(x) \right)_i
&=
\frac{\partial x_i}{\partial y_i} \left(
\frac{\partial f(x)}{\partial x_i}
- \frac{\partial f(x)}{\partial x_n} (u_{n-1} / u_i)
- \sum_{j=i+1}^{n-1} \frac{\partial f(x)}{\partial x_j}
z_j (u_{j-1} / u_{i}) \right) \\
&=
\frac{\partial x_i}{\partial y_i} \left(
\frac{\partial f(x)}{\partial x_i}
- \frac{\partial f(x)}{\partial x_n} (u_{n-1} / u_i)
- \frac{v_{n-1} - v_i}{u_i} \right) \\
\end{align*}

Now assume we have also computed a cumulative sum of $x$ in $O(n)$ time:

$$w_i \defeq \sum_{k=1}^i x_k$$

\begin{align}
&\left( \nabla_y f(x) \right)_i \\
&=
z_i (1 - z_i) \left(1 - \sum_{k=1}^{i-1} \right) \left(
\frac{\partial f(x)}{\partial x_i}
- \frac{\partial f(x)}{\partial x_n} (u_{n-1} / u_i)
- \frac{v_{n-1} - v_i}{u_i} \right) \\
&=
z_i (1 - z_i) (1 - w_{i-1} ) \left(
\frac{\partial f(x)}{\partial x_i}
- \frac{\partial f(x)}{\partial x_n} (u_{n-1} / u_i)
- \frac{v_{n-1} - v_i}{u_i} \right) \\
\end{align}

Finally, we see that $\left( \nabla_y f(x) \right)_i$ is computable in $O(1)$
given $\nabla_x f(x)$ as well as $u$, $v$, and $w$, each of which is computed in
$O(n)$ and does not depend on $i$. Therefore we can compute $\nabla_y f(x)$ in
$O(n)$.


\subsection{Gradients of $\log |\det J_{S}|$}


\section{Scaled simplex transformation}

Longer transcripts have more oppourtunities to observe reads. Expression must be
scaled to account for this effect, which is usually done by dividing by a
``effective length'' $\ell_i$ value for each transcript $i$. We define the
following transform $L: \Delta^{n-1} \mapsto \Delta^{n-1}$, where $\Delta^{n-1}$
is the $(n-1)$-simplex as,

$$
x'_i \defeq L(x)_i = \frac{\ell_i x_i}{\sum_{k=1}^{n} \ell_k x_k}
$$

Note that this is not an invertable transformation when applied to real numbers,
but is invertable over a simplex. The inverse is simply $$L^{-1}(x)_i =
\frac{(1/\ell_i) x_i}{\sum_{k=1}^{n} (1/\ell_k) x_k}$$

\subsection{Gradients}

The gradients of an objective function applied to transformed values ($\nabla_x
f(L(x))$) can be computed in $O(n)$.


The partial derivatives of $L$ are for all $i$,
$$\frac{\partial}{\partial x_i} \frac{\ell_i x_i}{\sum_{k=1}^{n} \ell_k x_k}
= \frac{\ell_i}{\sum_{k=1}^{n} \ell_k x_k}
- \frac{\ell_{i}^2 x_i}{\left( \sum_{k=1}^{n} \ell_k x_k \right)^2}$$
and for $i, j$ where $i \neq j$,
$$\frac{\partial}{\partial x_i} \frac{\ell_j x_j}{\sum_{k=1}^{n} \ell_k x_k}
= - \frac{\ell_i \ell_j x_i}{\left( \sum_{k=1}^{n} \ell_k x_k \right)^2}$$


The partial derivatives of $f(L(x))$, where $x'_j = (L(x))_j$, are,
\begin{align*}
\frac{\partial}{\partial x_i} f(L(x)) &=
    \sum_{j=1}^{n} \frac{\partial x'_j}{\partial x_i} \frac{\partial}{\partial x'_j} f(x') \\
&=
    \left(\frac{\ell_i}{\sum_{k=1}^{n} \ell_k x_k}
    - \frac{\ell_{i}^2 x_i}{\left( \sum_{k=1}^{n} \ell_k x_k \right)^2}\right)
    \frac{\partial}{\partial x'_i} f(x')
    - \sum_{\substack{j=1 \\ j \neq i}}^{n}
\frac{\ell_i \ell_j x_i}{\left( \sum_{k=1}^{n} \ell_k x_k \right)^2}
    \frac{\partial}{\partial x'_j} f(x') \\
    &=
\frac{\ell_i}{\sum_{k=1}^{n} \ell_k x_k}  \frac{\partial}{\partial x'_i} f(x')
- \ell_i \sum_{j=1}^{n}
\frac{\ell_j x_i}{\left( \sum_{k=1}^{n} \ell_k x_k \right)^2}
    \frac{\partial}{\partial x'_j} f(x') \\
\end{align*}

Note that both sums that appear do not depend on $i$, so they can be computed
once and reused for each $i$ in $O(n)$, and only an constant amount of
additional work is needed for each $i$, therefore the gradient is computable in
linear time.

\subsection{Jacobian}

The determinant of the Jacobian matrix of the $L$ transform is 0, which we prove
here.

The diagonal entries of the Jacobian matrix are, for $1 \le i \le n$,

$$J_{ii} = \frac{\partial}{\partial x_i} \frac{\ell_i x_i}{\sum_{k=1}^{n} \ell_k x_k}
= \frac{\ell_i}{\sum_{k=1}^{n} \ell_k x_k}
- \frac{\ell_{i}^2 x_i}{\left( \sum_{k=1}^{n} \ell_k x_k \right)^2}$$

Every off-diagonal entry, for $i, j$ where $i \neq j$ is,
$$J_{ij} = \frac{\partial}{\partial x_i} \frac{\ell_j x_j}{\sum_{k=1}^{n} \ell_k x_k}
= - \frac{\ell_i \ell_j x_i}{\left( \sum_{k=1}^{n} \ell_k x_k \right)^2}$$

Note that $J$ can be factored as $J = u v^{T} + I u$ where for each $1 \le i
\le n$,

$$ u_i = \frac{\ell_i}{\sum_{k=1}^{n} \ell_k x_k} $$

$$ v_i = \frac{-\ell_i x_i}{\sum_{k=1}^{n} \ell_k x_k} $$

Now the Matrix Determinant Lemma can be applied, giving us,
\begin{align*}
\det(J) &= \det(u v^{T} + I u) \\
        &= (1 + v^T (Iu)^{-1} u) \det(Iu) \\
        &= \left(1 + \sum_{i=1}^{n} u_i v_i (1/u_i) \right) \det(Iu) \\
        &= \left(1 + \sum_{i=1}^{n} v_i \right) \det(Iu) \\
        &= \left(1 - \sum_{i=1}^{n} \frac{\ell_i x_i}{\sum_{k=1}^{n} \ell_k x_k}
        \right) \det(Iu) \\
        &= 0
\end{align*}

The final step follows because $\sum_{i=1}^{n} \frac{\ell_i x_i}{\sum_{k=1}^{n}
\ell_k x_k} = 1$.



\end{document}

